{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550731ed-f309-4c75-bc7f-94cd1a098a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **PyTorch: Internal Feature Map Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e05998e-765a-4541-8abb-db621e876404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd6d8d2-478a-4a56-82e9-c6cd0e088cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pickle\n",
    "def save_pickle(filename, obj):\n",
    "    with open(str(filename), 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(\"Object saved\")\n",
    "        \n",
    "# load pickle\n",
    "def load_pickle(filename):\n",
    "    with open(str(filename), 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    print(\"Object loaded\")\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccef898b-6406-4843-bdb5-f24c2b8a124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IMG SIZE = torch.Size([1, 3, 448, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flfo\\Anaconda3\\envs\\CNN_Vis\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import YoloDemonstration\n",
    "model, device = YoloDemonstration.load_model('yolov7.pt')\n",
    "pred, tensor_collection = YoloDemonstration.img_prediction('input/cat.jpg', model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f91f0ef-65ab-40bc-9932-8fe753c8a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded\n"
     ]
    }
   ],
   "source": [
    "tensor_collection = load_pickle('tensor_collection.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64e8be2-ca84-4418-980d-67ac2e07e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= tensor_collection[0][0][0].detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e194a920-a6e8-41b9-a441-f75fb49202d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 448, 640])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c486a4d5-2bec-4ce7-900a-1c9047235b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "484e9522-98a1-434a-bf34-782bc1d23899",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0aa68897-09a8-45fb-9bdb-b72a4ce8ad8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([448, 640])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b4bd0a0-4985-407f-b7f1-22e41b1ebf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.07476)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140a11e-854b-41bf-b536-abe9c1668f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([255], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0c01d74-f30d-4789-a919-b30e8024fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grayscale(layer_map, norm_val):\n",
    "    \"\"\"\n",
    "    Calculates normalized matrices in an array of 2D arrays and maps them to grayscale\n",
    "    Input: NumPy array with 3 dimensions (num_pic, width, height)\n",
    "    Output: modified NumPy array with same 3 dimensions\n",
    "    \"\"\"\n",
    "    #start = time.time()\n",
    "    #norm_val = torch.as_tensor([255], dtype=torch.float32)\n",
    "    \n",
    "    for i in range(layer_map.size()[0]):\n",
    "        #img = test_map[i,:,:]\n",
    "        img = layer_map[i]\n",
    "        #print(\"TEST\")\n",
    "        v_min = img.min()\n",
    "        v_max = img.max()\n",
    "        img -= v_min\n",
    "        img /= (v_max - v_min)\n",
    "        img *= norm_val\n",
    "   \n",
    "    return layer_map\n",
    "\n",
    "\n",
    "def calc_grayscale_dataset(data_dict, layer_nums=[]):\n",
    "    \"\"\"\n",
    "    For whole data set as PyTorch Yolo tensor:\n",
    "    Calculates normalized matrices in a list of 2D arrays and maps them to grayscale\n",
    "    \n",
    "    Input: User-defined tensor collection library added to the Yolo Model as attribute\n",
    "    Output: Dictionary with layer number as key and corresponding normalized feature maps\n",
    "    \"\"\"\n",
    "    \n",
    "    ret_dict = {}\n",
    "    norm_val = torch.as_tensor([255], dtype=torch.float32)\n",
    "    \n",
    "    if layer_nums:\n",
    "        keys = layer_nums\n",
    "    else:\n",
    "        keys = data_dict.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        # get PyTorch Yolo tensor with 3 dimensions (num_pics, width, height)\n",
    "        # transform to NumPy array\n",
    "        ret_dict[key] = calc_grayscale(data_dict[key][0][0].detach(), norm_val) \n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2d9bc429-0666-44c5-bbbe-9ceb22ec7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collection = tensor_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc247fd0-8664-4027-adb4-acdacc4c3946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumed: 1153.9371013641357 ms\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "#layer_nums = [0,1,2,30,60,90,100,104]\n",
    "layer_nums = []\n",
    "s = time.time()\n",
    "ret_dict = calc_grayscale_dataset(test_collection, layer_nums=layer_nums)\n",
    "e = time.time()\n",
    "print(f\"Time consumed: {(e-s) * 1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0de6090c-6952-465a-a9a0-844ff8334a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 448, 640])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_dict[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15a4167-2d9d-42b5-852e-00f88fbd5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc3fe2c-09b4-4de8-bb50-6582f8b05cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.conv2d>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32687464-8d51-43b5-bc25-97bce55c371f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
